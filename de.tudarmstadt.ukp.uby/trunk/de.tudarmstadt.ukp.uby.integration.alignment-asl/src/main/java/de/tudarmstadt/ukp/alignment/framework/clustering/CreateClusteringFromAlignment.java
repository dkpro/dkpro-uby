package de.tudarmstadt.ukp.alignment.framework.clustering;

import java.io.BufferedReader;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.PrintStream;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.HashMap;
import java.util.HashSet;

import de.tudarmstadt.ukp.alignment.framework.Global;
import de.tudarmstadt.ukp.alignment.framework.graph.OneResourceBuilder;
import de.tudarmstadt.ukp.lmf.model.enums.ELanguageIdentifier;

public class CreateClusteringFromAlignment
{

	public static void main(String[] args) throws ClassNotFoundException, SQLException, IOException
	{
		/* GLOBAL SETTINGS */

		Global.init();
		String language = ELanguageIdentifier.ENGLISH; 
		
		/*RESOURCE 1*/

		boolean synset1 = true;
		boolean usePos1 = true;
		int prefix1 = Global.WN_Synset_prefix; 	 
		 
	    OneResourceBuilder bg_1 = new OneResourceBuilder("uby_release_1_0","root","fortuna", prefix1,language,synset1,usePos1);
	    String alignment_file ="/home/local/UKP/matuschek/ClusterEvaluationTM/WN_OW_full_joint.csv";
	   
		splitSynsetsToSenses(bg_1, alignment_file,true);
		produceClustersFromSenseAlignment(alignment_file+"_sense");
		splitClustersByLemma(bg_1,alignment_file+"_sense_cluster",true);
		mergeClustersWithSharedSenses(alignment_file+"_sense_cluster_cleaned");
		checkPOSpurity(bg_1,alignment_file+"_sense_cluster_cleaned_merged");
		checkForDupesInSameCluster(alignment_file+"_sense_cluster_cleaned_merged_POScleaned");
		filterByLexemeList(bg_1,alignment_file+"_sense_cluster_cleaned_merged_POScleaned_noDupes","/home/local/UKP/matuschek/ClusterEvaluationTM/lemmas.tsv",true);
		bringWNextRefClustersIntoTMFormat("/home/local/UKP/matuschek/ClusterEvaluationTM/WN_WP_full_alignment_DijkstaWSA_best_corrected_senses.txt_cluster_cleaned_merged_POScleaned_noDupes_lexemeFiltered");

}	
	/**
	 * This method splits sense clusters which contain senses of different POS. This might occur if you align against a resource which
	 * lacks POS information, such as OmegaWiki.
	 * 
	 * 
	 * @param bg_1 The resource
	 * @param infile The sense cluster file
	
	 */
	public static void checkPOSpurity(OneResourceBuilder bg_1, String infile)
	{
		
		try
		{
		Connection connection = bg_1.connection;
		FileReader in = new FileReader(infile);
		 BufferedReader input =  new BufferedReader(in);
		FileOutputStream outstream;
		PrintStream p;
		outstream = new FileOutputStream(infile+"_POScleaned");
		p = new PrintStream( outstream );
		Statement statement = connection.createStatement();
		HashMap<String,String> idPosMap = new HashMap<String, String>();
		ResultSet rs =	statement.executeQuery("SELECT externalReference,partOfSpeech FROM MonolingualExternalRef join LexicalEntry join  Sense where Sense.senseId like '"+bg_1.prefix_string+"%' and MonolingualExternalRef.senseID = Sense.senseId and Sense.lexicalEntryId = LexicalEntry.lexicalEntryId");
		while(rs.next())
		{
			idPosMap.put(rs.getString(1), rs.getString(2));
		}

		String line;
		int i =0;
		 while((line =input.readLine())!=null)
		 {
			String[] elements = line.split("\t");
			String results = "";
			HashSet<String> pos_count = new HashSet<String>();
			HashMap<String,HashSet<String>> pos_map = new HashMap<String, HashSet<String>>();
			for (String s : elements)
			{
				String pos = idPosMap.get(s);
				if(!pos_map.containsKey(pos))
				{
					pos_map.put(pos, new HashSet<String>());
				}
				pos_map.get(pos).add(s);
				pos_count.add(idPosMap.get(s));
			}
			for (String x: pos_map.keySet())
			{
				results = "";
				HashSet<String> posCluster = pos_map.get(x);
				if(posCluster.size()==1) {
					continue;
				}
				for (String id: posCluster)
				{
					results+=id+"\t";
				}
				p.println(results.trim());
			}
			
			if(pos_count.size()>1) {	//For debugging purposes
				System.out.println(line);
			}
			
		 }
		 p.close();
			in.close();
		}
		catch(Exception e)
		{
			e.printStackTrace();
		}
		
	}
	

	
	

	/**
	 * This method separates clusters with mixed lemmas
	 * 
	 * 
	 * @param bg_1 the resource
	 * @param infile the (mixed lemma) sense clusters
	 * @param extRef are we using external References or not?
	 * @throws SQLException
	 * @throws IOException
	 * @throws ClassNotFoundException
	 */
	public static void splitClustersByLemma(OneResourceBuilder bg_1, String infile, boolean extRef) throws SQLException, IOException, ClassNotFoundException
	{
		FileOutputStream outstream;
		PrintStream p;
		outstream = new FileOutputStream(infile+"_cleaned");
		p = new PrintStream( outstream );
		FileReader in = new FileReader(infile);;
		String line;
		Statement statement = bg_1.connection.createStatement();
		HashMap<String,String> extRefLemmaMap= new HashMap<String, String>();
		
		BufferedReader input =  new BufferedReader(in);
		ResultSet rs;

		if(extRef)
		{
			rs =	statement.executeQuery("SELECT externalReference, writtenForm FROM LexicalEntry join FormRepresentation_Lemma join Sense join MonolingualExternalRef where FormRepresentation_Lemma.lemmaId = LexicalEntry.lemmaId and Sense.lexicalEntryId = LexicalEntry.lexicalEntryId and MonolingualExternalRef.senseId = Sense.senseId and Sense.senseId like '"+bg_1.prefix_string+"%'");
			
		}
		else
		{
			 rs =	statement.executeQuery("SELECT  senseId, writtenForm FROM  LexicalEntry     join   FormRepresentation_Lemma      join  Sense where FormRepresentation_Lemma.lemmaId = LexicalEntry.lemmaId  and Sense.lexicalEntryId = LexicalEntry.lexicalEntryId and Sense.senseId like '"+bg_1.prefix_string+"%'");
		}
		while(rs.next())
		{
			extRefLemmaMap.put(rs.getString(1), rs.getString(2));
		}
			while((line = input.readLine()) != null)
			{
				 line = line.trim();
				 String[] sensekeys = line.split("\t");
				 HashMap<String,HashSet<String>> lemmaIDMap= new HashMap<String, HashSet<String>>();
				 for(String key : sensekeys)
				 {
					 String lemma = extRefLemmaMap.get(key);					 
					 if(lemmaIDMap.get(lemma) == null)
					 {
						 lemmaIDMap.put(lemma, new HashSet<String>());
					 }
					 lemmaIDMap.get(lemma).add(key);
				 }
				 for (String l : lemmaIDMap.keySet())
				 {
					String output = "";
					if(lemmaIDMap.get(l).size()>1) {
						for (String k: lemmaIDMap.get(l))
						{
							output+=k+"\t";
						}
						p.println(output.trim());
					}

			 }
			 }
			p.close();

	}
	
	
	
	/**
	 * This method takes a given SYNSET alignment file (as produced by the framework) and splits it into sense alignments
	 * 
	 * 
	 * 
	 * @param bg_1 The resource to be clustered
	 * @param infile The alignment file
	 * @param extRef Is the alignment file given in external References or UBY ids?
	 * @throws ClassNotFoundException
	 * @throws SQLException
	 * @throws IOException
	 */

	public static void splitSynsetsToSenses(OneResourceBuilder bg_1, String infile, boolean extRef)
	{
		try {
			Connection connection = bg_1.connection;
			FileReader in = new FileReader(infile);
			BufferedReader input =  new BufferedReader(in);
			FileOutputStream outstream;
			PrintStream p;
			outstream = new FileOutputStream(infile+"_sense");
			p = new PrintStream( outstream );
			Statement statement = connection.createStatement();
			HashMap<String,String > idMapSense = new HashMap<String, String>();
			HashMap<String,String > idMapSynset = new HashMap<String, String>();
			HashMap<String, HashSet<String>> senseSynsetMapping = new HashMap<String, HashSet<String>>();
			ResultSet rs;
			if(extRef)
			{
			rs =	statement.executeQuery("SELECT externalReference, senseId FROM uby_release_1_0.MonolingualExternalRef where senseId like  '"+bg_1.prefix_string+"%'");
			while(rs.next())
			{
				idMapSense.put( rs.getString(2), rs.getString(1));
			}
			rs =	statement.executeQuery("SELECT externalReference, synsetId FROM uby_release_1_0.MonolingualExternalRef where synsetId like '"+bg_1.prefix_string+"%'");
			while(rs.next())
			{
				idMapSynset.put(rs.getString(1),rs.getString(2));
			}
			}
			rs =	statement.executeQuery("SELECT synsetId, senseId FROM uby_release_1_0.Sense where synsetId like '"+bg_1.prefix_string+"%'");
			while(rs.next())
			{
				if(!senseSynsetMapping.containsKey(rs.getString(1)))
				{
					senseSynsetMapping.put(rs.getString(1), new HashSet<String>());
				}
				senseSynsetMapping.get(rs.getString(1)).add(rs.getString(2));
			}

			String line;
			 while((line =input.readLine())!=null)
			 {

				String[] ids = line.split("\t");
				String synsetId = ids[0];
				String synsetUbyId;
				if(extRef)
				{
					synsetUbyId = idMapSynset.get(synsetId);
				}
				else
				{
					synsetUbyId = synsetId;
				}
				HashSet<String> ss = senseSynsetMapping.get(synsetUbyId);
						
				for (String s : ss)
				{
					if(extRef)
						p.println(idMapSense.get(s)+"\t"+ids[1]);
					else
						p.println(s+"\t"+ids[1]);
				}
				
			 }
			 p.close();
			 input.close();
		} catch (Exception e) {
			e.printStackTrace();
		}

	}
	/**
	 * This method naively clusters those senses together which have the same alignment target. Further downstream cleaning will be necessary
	 * 
	 * 
	 * @param infile The sense alignment file
	 * 
	 */
	
	
	public static void produceClustersFromSenseAlignment(String infile)
	{
		
		try {
			HashMap<String,HashSet<String> > clusters = new HashMap<String, HashSet<String>>();
			FileReader in = new FileReader(infile);
			BufferedReader input =  new BufferedReader(in);
			 String line = "";
			while((line = input.readLine())!=null)
			{
				String[] ids = line.split("\t");
				if(!clusters.containsKey(ids[1]))
				{
					clusters.put(ids[1],new HashSet<String>());
				}
				(clusters.get(ids[1])).add(ids[0]);
				}

			FileOutputStream outstream = new FileOutputStream(infile+"_cluster");
			PrintStream p = new PrintStream(outstream);
			for (String key : clusters.keySet())
			{
				HashSet<String> sss = clusters.get(key);
				if(sss.size()>1)
				{
				for (String s : sss)
				{
					p.print(s+"\t");
				}
				p.println();
				}
			}
			input.close();

			p.close();
			in.close();
		} catch (Exception e) {
 
			e.printStackTrace();
		}
	}


	/**
	 * 
	 * This simple method just removes the POS information as given in the External References for WordNet senses
	 * 
	 * E.g. [POS: noun] acid%1:06:00:: -> acid%1:06:00:: 
	 * 
	 * @param input_file The sense clustering
	 */
	
	public static void bringWNextRefClustersIntoTMFormat(String input_file)
	{
		try
		{
		FileOutputStream outstream;
		PrintStream p;
		outstream = new FileOutputStream(input_file+"_TM");
		p = new PrintStream( outstream );
		FileReader in = new FileReader(input_file);
		BufferedReader input =  new BufferedReader(in);		
		String line ="";
		while ((line = input.readLine())!=null)
		{
			String out="";
			String[] ids = line.split("\t");
			for(String id : ids)
			{
				out+=(id.split("] ")[1]+"\t");
				
			}
			p.println(out.trim());
		}	
		p.close();
		in.close();
		}
		catch(Exception e)
		{
			e.printStackTrace();
		}
		
	}

	
	/**
	 * This method filters out senses clusters accorind to a given, tab-separated lemma/POS list, where one lexeme per line is given, e.g.
	 * 
	 * fish	noun
	 * swim	verb
	 * 
	 * 
	 * @param bg_1 The resource 
	 * @param input_file The sense clustering
	 * @param lexeme_file The tab-separated lexeme list
	 * @param extRef Do we have external references or UBY IDs?
	 */
	public static void filterByLexemeList(OneResourceBuilder bg_1, String input_file, String lexeme_file, boolean extRef)  {
		
		try
		{
		Statement statement = bg_1.connection.createStatement();
		HashMap<String,String> extRefLemmaMap= new HashMap<String, String>();
		ResultSet rs;
		if(extRef)
		{
			rs =	statement.executeQuery("SELECT externalReference, writtenForm, partOfSpeech FROM LexicalEntry join FormRepresentation_Lemma join Sense join MonolingualExternalRef where FormRepresentation_Lemma.lemmaId = LexicalEntry.lemmaId and Sense.lexicalEntryId = LexicalEntry.lexicalEntryId and MonolingualExternalRef.senseId = Sense.senseId and Sense.senseId like '"+bg_1.prefix_string+"%'");
			
		}
		else
		{
			 rs =	statement.executeQuery("SELECT  senseId, writtenForm, partOfSpeech FROM  LexicalEntry     join   FormRepresentation_Lemma      join  Sense where FormRepresentation_Lemma.lemmaId = LexicalEntry.lemmaId  and Sense.lexicalEntryId = LexicalEntry.lexicalEntryId and Sense.senseId like '"+bg_1.prefix_string+"%'");
		}
		while(rs.next())
		{
			extRefLemmaMap.put(rs.getString(1), rs.getString(2)+"\t"+rs.getString(3));
		}
		FileReader in = new FileReader(lexeme_file);
		BufferedReader input =  new BufferedReader(in);
		HashSet<String> lexemes = new HashSet<String>();
		String line ="";
		FileOutputStream outstream;
		PrintStream p;
		outstream = new FileOutputStream(input_file+"_lexemeFiltered");
		p = new PrintStream( outstream );
		while ((line = input.readLine())!=null)
		{
			lexemes.add(line.replace(" ","_")); //Assuming spaces are represented by underscores in the lexeme list
		}
		in.close();
		in = new FileReader(input_file);
		input =  new BufferedReader(in);
		while ((line = input.readLine())!=null)
		{
			
			String[] ids = line.split("\t");
			String lemmaPos= extRefLemmaMap.get(ids[0]);
			if(lexemes.contains(lemmaPos))		
			{
				p.println(line);
			}

		}
		p.close();
		in.close();
		}
		catch(Exception e)
		{
			e.printStackTrace();
		}
	
	}

	/**
	 * This method checks for duplicate senses in the same cluster and removes them
	 * 
	 *  
	 * @param input_file the clustering
	 * @throws ClassNotFoundException
	 * @throws SQLException
	 * @throws IOException
	 */
	public static void checkForDupesInSameCluster(String input_file) 
	{
		try
		{
		FileReader in = new FileReader(input_file);
		BufferedReader input =  new BufferedReader(in);
		HashSet<String> alreadySeen2 = new HashSet<String>();
		String line ="";
		FileOutputStream outstream;
		PrintStream p;
		outstream = new FileOutputStream(input_file+"_noDupes");
		p = new PrintStream( outstream );
		while ((line = input.readLine())!=null)
		{
			String out = "";
			String[] ids = line.split("\t");
			int c = 0;
			for(String id : ids)
			{
				if(alreadySeen2.contains(id)) {
					continue;
				}
				alreadySeen2.add(id);
				out+=id+"\t";
				c++;
			}
			if(c<2) {
				continue;
			}

			p.println(out.trim());
		}
		p.close();
		in.close();
		}
		catch(Exception e)
		{
			e.printStackTrace();
		}
		}


	
	/**
	 * 
	 * This method iteratively merges clusters which share senses, until the number of clusters stays stable
	 * 
	 * @param input_file the sense cluster file
	 * 
	 * 
	 */
	public static void mergeClustersWithSharedSenses(String input_file)
	{
		
		
		try {
			HashSet<String> input_clusters = new HashSet<String>();
			HashSet<String> output_clusters = new HashSet<String>();
			FileReader in = new FileReader(input_file);
			BufferedReader input =  new BufferedReader(in);
			String l ="";
			while ((l = input.readLine())!=null)
			{
				input_clusters.add(l);
			}
			input.close();
			int count = 0;
			int cluster_size = input_clusters.size();
			while(output_clusters.size()<cluster_size)
			{ 
				output_clusters = new HashSet<String>();
				cluster_size = input_clusters.size();			
				HashMap<String, HashSet<String>> reverse_candidates = new HashMap<String, HashSet<String>>();
				HashMap<String, HashSet<String>> candidates = new HashMap<String, HashSet<String>>();
				HashSet<String> alreadySeen2 = new HashSet<String>();
			for (String line : input_clusters)
			{
				if(alreadySeen2.contains(line)) {
					continue;
				}
				alreadySeen2.add(line);
				count++;
				String[] elements = line.split("\t");
				for (String id : elements)
				{
					if(!candidates.containsKey(id)) {
						candidates.put(id, new HashSet<String>());
					}
					if(!reverse_candidates.containsKey(count+"")) {
						reverse_candidates.put(count+"", new HashSet<String>());
					}
					reverse_candidates.get(count+"").add(id);
					candidates.get(id).add(count+"");
				}
			}

			HashSet<String> alreadySeen = new HashSet<String>();
			HashMap<String, HashSet<String>> reverse_candidates_merged = new HashMap<String, HashSet<String>>();

			for(String id_mirror : reverse_candidates.keySet())
			{
				if(alreadySeen.contains(id_mirror)) {
					continue;
				}
				reverse_candidates_merged.put(id_mirror, new HashSet<String>());
				alreadySeen.add(id_mirror);
				HashSet<String> rev_cluster = reverse_candidates.get(id_mirror);
				for(String sense : rev_cluster)
				{
					HashSet<String> cluster =	candidates.get(sense);
					reverse_candidates_merged.get(id_mirror).add(sense);

					cluster.remove(id_mirror);
					if(rev_cluster.size()==0)
					{
						//do nothing
					}
					else
					{
						for(String remaining : cluster)
						{
							reverse_candidates_merged.get(id_mirror).addAll(reverse_candidates.get(remaining));
							alreadySeen.add(remaining);
						}
					}

				}
				}

			for(String id_mirror : reverse_candidates_merged.keySet())
			{

				HashSet<String> cluster = reverse_candidates_merged.get(id_mirror);
				if(cluster.size() == 1) {
					continue;
				}
				String output_cluster = "";
				for(String sense : cluster)
				{
					output_cluster+=sense+"\t";
				}
				output_cluster=output_cluster.trim();
				if(alreadySeen.contains(output_cluster))
				{
					continue;
				}
				else
				{
					alreadySeen.add(output_cluster);
				}
				output_clusters.add(output_cluster);
			}
			input_clusters = output_clusters;
			
			}
			FileOutputStream outstream;
			PrintStream p;
			outstream = new FileOutputStream(input_file+"_merged");
			p = new PrintStream( outstream );
			for(String s : output_clusters)
			{
				p.println(s);
			}
			p.close();
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	
}

